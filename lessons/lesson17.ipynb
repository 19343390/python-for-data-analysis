{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson 17 - Statistics Packages\n",
    "\n",
    "In this lesson we'll consider some of the various statistics tools available in Python. Many packages provide statistical support: Pandas, Numpy, Scipy, and Scikit-Learn.\n",
    "\n",
    "You will probably have to install `scikit-learn` (`sklearn`) and `outlier_utils` (`outliers`) before proceeding:\n",
    "\n",
    "```\n",
    "conda install scikit-learn\n",
    "pip install outlier_utils\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named seaborn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f76a06ee5b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mskbio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mu'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named seaborn"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "from outliers import smirnov_grubbs as grubbs\n",
    "import skbio\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pandas and seaborn environments\n",
    "pd.set_option('display.max_rows', 25)\n",
    "sns.set()\n",
    "sns.set_context('notebook')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic stats with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A large number of methods for computing descriptive statistics and other related operations on Series, DataFrame, and Panel. Most of these are aggregations (hence producing a lower-dimensional result) like `sum()`, `mean()`, and `quantile()`, but some of them, like `cumsum()` and `cumprod()`, produce an object of the same size. Generally speaking, these methods take an axis argument, just like `ndarray.{sum, std, ...}`, but the axis can be specified by name or integer:\n",
    "\n",
    "* Series: no axis argument needed\n",
    "* DataFrame: “index” (axis=0, default), “columns” (axis=1)\n",
    "* Panel: “items” (axis=0), “major” (axis=1, default), “minor” (axis=2)\n",
    "\n",
    "Function | Description\n",
    "--------- | ----------\n",
    "count | Number of non-null observations\n",
    "sum | Sum of values\n",
    "mean | Mean of values\n",
    "mad | Mean absolute deviation\n",
    "median | Arithmetic median of values\n",
    "min | Minimum\n",
    "max | Maximum\n",
    "mode | Mode\n",
    "abs | Absolute Value\n",
    "prod | Product of values\n",
    "std | Bessel-corrected sample standard deviation\n",
    "var | Unbiased variance\n",
    "sem | Standard error of the mean\n",
    "skew | Sample skewness (3rd moment)\n",
    "kurt | Sample kurtosis (4th moment)\n",
    "quantile | Sample quantile (value at %)\n",
    "cumsum | Cumulative sum\n",
    "cumprod | Cumulative product\n",
    "cummax | Cumulative maximum\n",
    "cummin | Cumulative minimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Monthly precipitation in La Jolla from 2008 to 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the data: covert to datetime, average precipitation per month, get month and year, reset index\n",
    "df = pd.read_csv('../data/la_jolla_precip_monthly.csv')\n",
    "df['DATE'] = pd.to_datetime(df['DATE'])\n",
    "df = df.groupby('DATE').mean()\n",
    "df['MONTH'] = [x.month for x in df.index]\n",
    "df['YEAR'] = [x.year for x in df.index]\n",
    "df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the resulting dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# explore the data using a plot\n",
    "plt.plot(df.DATE, df.PRCP);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure our index is sequential\n",
    "plt.plot(df.index, df.PRCP);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# describe\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean\n",
    "df.PRCP.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# std\n",
    "df.PRCP.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantile\n",
    "df.PRCP.quantile(0.25), df.PRCP.quantile(0.5), df.PRCP.quantile(0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min\n",
    "df.PRCP.min(), df.PRCP.idxmin()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max (with rounding)\n",
    "df.PRCP.max(), df.PRCP.max().round(), df.PRCP.idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumsum\n",
    "df.PRCP.cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# value_counts\n",
    "df.PRCP.round().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='MONTH', y='PRCP', data=df, order=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.regplot(x='MONTH', y='PRCP', data=df, order=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seaborn is handy to generate plots, but it doesn't provide easy access to the coefficients. For more control, we can use Numpy, Scipy, and other statistics packages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with Numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### np.polyfit - least squares polynomial fit (1st order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1st order with np.polyfit\n",
    "m, b = np.polyfit(df.MONTH, df.PRCP, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot scatter and polyfit\n",
    "plt.scatter(df.MONTH, df.PRCP)\n",
    "plt.plot(df.MONTH, m*df.MONTH + b, '-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort DataFrame by month, then re-plot\n",
    "df.sort_values('MONTH', inplace=True)\n",
    "plt.scatter(df.MONTH, df.PRCP)\n",
    "plt.plot(df.MONTH, m*df.MONTH + b, '-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### np.polyfit - least squares polynomial fit (2nd order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd order with np.polyfit\n",
    "p = np.polyfit(df.MONTH, df.PRCP, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# values of p are in decending orders\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create a finely spaced array\n",
    "x1 = np.linspace(1,12)\n",
    "x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the y vector for the fit curve \n",
    "y1 = np.polyval(p, x1)\n",
    "y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.MONTH, df.PRCP)\n",
    "plt.plot(x1, y1, '-');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3rd order with np.polyfit\n",
    "p = np.polyfit(df.MONTH, df.PRCP, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = np.polyval(p, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(df.MONTH, df.PRCP)\n",
    "plt.plot(x1, y1, '-');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting time series to a sinusoidal wave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sine wave refresher\n",
    "period = 4*np.pi\n",
    "freq = (2*np.pi)/period\n",
    "phase = 0\n",
    "amplitude = .5\n",
    "offset = 1\n",
    "x1 = np.linspace(0, 24, num=2000)\n",
    "y1 = np.sin(x1 * freq + phase) * amplitude + offset\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x1, y1)\n",
    "ax.set_xticks([2*np.pi, 4*np.pi, 6*np.pi])\n",
    "ax.set_xticklabels(['2$\\pi$', '4$\\pi$', '6$\\pi$']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are going to use the numerical index for our \"t\" variable\n",
    "# make sure our index is sequential (December 2008 is zero)\n",
    "df.sort_index(inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df.index, df.PRCP)\n",
    "plt.xticks(np.arange(0, df.index.max(), 12));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store our values as new variables\n",
    "t = df.index\n",
    "data = df.PRCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# guess the sine wave properties\n",
    "guess_period = 12\n",
    "guess_freq = (2*np.pi)/guess_period\n",
    "guess_phase = 0\n",
    "guess_amplitude = 10\n",
    "guess_offset = 10\n",
    "\n",
    "p0 = [guess_freq, guess_amplitude, guess_phase, guess_offset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the function we want to fit\n",
    "def my_sin(x, freq, amplitude, phase, offset):\n",
    "    return np.sin(x * freq + phase) * amplitude + offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now do the fit\n",
    "fit = scipy.optimize.curve_fit(my_sin, t, data, p0=p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use this to plot our first estimate. This might already be good enough for you\n",
    "data_first_guess = my_sin(t, *p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate the fitted curve using the optimized parameters\n",
    "data_fit = my_sin(t, *fit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(data, '.')\n",
    "plt.plot(data_fit, label='after fitting')\n",
    "plt.plot(data_first_guess, label='first guess')\n",
    "plt.xticks(np.arange(0, df.index.max(), 12))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regression with Scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Moons of the Solar System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moons = pd.read_excel('../data/moons.xlsx')\n",
    "df_planets = pd.read_excel('../data/planets.xlsx')\n",
    "df_solar = pd.merge(df_moons, df_planets, left_on='planet_name', right_on='planet_name')\n",
    "df_solar['moon_volume_km3'] = 4/3*np.pi*(df_solar.moon_diameter_km/2)**3\n",
    "df_solar['planet_volume_km3'] = 4/3*np.pi*(df_solar.planet_diameter_km/2)**3\n",
    "df_solar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pearson correlation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.pearsonr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson correlation (linear regression on values) of diameters\n",
    "pearson_r_diameter, pearson_p_diameter = scipy.stats.pearsonr(df_solar.planet_diameter_km, df_solar.moon_diameter_km)\n",
    "pearson_r_diameter, pearson_p_diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pearson correlation (linear regression on values) of volumes\n",
    "pearson_r_volume, pearson_p_volume = scipy.stats.pearsonr(df_solar.planet_volume_km3, df_solar.moon_volume_km3)\n",
    "pearson_r_volume, pearson_p_volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Spearman correlation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.spearmanr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spearman correlation (linear regression on ranks) of diameters\n",
    "spearman_r_diameter, spearman_p_diameter = scipy.stats.spearmanr(df_solar.planet_diameter_km, df_solar.moon_diameter_km)\n",
    "spearman_r_diameter, spearman_p_diameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spearman correlation (linear regression on ranks) of volumes\n",
    "spearman_r_volume, spearman_p_volume = scipy.stats.spearmanr(df_solar.planet_volume_km3, df_solar.moon_volume_km3)\n",
    "spearman_r_volume, spearman_p_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# plot linear regressions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "sns.regplot(x='planet_diameter_km', y='moon_diameter_km', data=df_solar, ax=ax[0])\n",
    "sns.regplot(x='planet_volume_km3', y='moon_volume_km3', data=df_solar, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we consider the *ratio* of moon size to planet size?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate moon diameters and volumes relative to host planets\n",
    "df_solar['moon_planet_relative_diameter'] = df_solar.moon_diameter_km/df_solar.planet_diameter_km\n",
    "df_solar['moon_planet_relative_volume'] = df_solar.moon_volume_km3/df_solar.planet_volume_km3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot scatter plots\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "\n",
    "ax[0].scatter(df_solar.moon_diameter_km, df_solar.moon_planet_relative_diameter)\n",
    "ax[0].set_xlabel('moon_diameter_km')\n",
    "ax[0].set_ylabel('moon_planet_relative_diameter')\n",
    "\n",
    "ax[1].scatter(df_solar.moon_volume_km3, df_solar.moon_planet_relative_volume)\n",
    "ax[1].set_xlabel('moon_volume_km3')\n",
    "ax[1].set_ylabel('moon_planet_relative_volume')\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot distributions\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,5))\n",
    "sns.distplot(df_solar.moon_planet_relative_diameter, rug=True, bins=20, ax=ax[0])\n",
    "sns.distplot(df_solar.moon_planet_relative_volume, hist=False, kde=False, rug=True, bins=20, ax=ax[1]);\n",
    "ax[1].set_xscale('log')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grubbs's test for outliers\n",
    "\n",
    "Grubbs's test is used to detect outliers in a univariate data set assumed to come from a normally distributed population.\n",
    "\n",
    "`outlier_utils` is a library for detecting and removing outliers using Grubbs's test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print outliers from the dataset - relative diameter\n",
    "grubbs.max_test_outliers(df_solar.moon_planet_relative_diameter, alpha=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers - relative diameter\n",
    "moon_planet_rel_diam_no_outliers = grubbs.test(df_solar.moon_planet_relative_diameter, alpha=0.05)\n",
    "moon_planet_rel_diam_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print outliers from the dataset - relative volume (higher alpha)\n",
    "grubbs.max_test_outliers(df_solar.moon_planet_relative_volume, alpha=0.000005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers - relative volume (higher alpha)\n",
    "moon_planet_rel_vol_no_outliers = grubbs.test(df_solar.moon_planet_relative_volume, alpha=0.000005)\n",
    "moon_planet_rel_vol_no_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests of indpendence (of two nominal variables)\n",
    "\n",
    "Source: [Handbook of Biological Statistics](http://www.biostathandbook.com) by John H. McDonald\n",
    "\n",
    "Test | Purpose | Notes | Example\n",
    "----- | ----- | ----- | -----\n",
    "Fisher's exact test | Test hypothesis that proportions are the same in different groups | Use for small sample sizes (less than 1000) | Count the number of live and dead patients after treatment with drug or placebo, test the hypothesis that the proportion of live and dead is the same in the two treatments, total sample <1000\n",
    "Chi-square test of independence | Test fit of observed frequencies to expected frequencies | Use for large sample sizes (greater than 1000) | Count the number of live and dead patients after treatment with drug or placebo, test the hypothesis that the proportion of live and dead is the same in the two treatments, total sample >1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fisher's exact test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Fisher's exact test of independence when you have two nominal variables and you want to see whether the proportions of one variable are different depending on the value of the other variable. Use it when the sample size is small."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.fisher_exact?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "* table: array_like of ints.\n",
    "    A 2x2 contingency table.  Elements should be non-negative integers.\n",
    "* alternative: {'two-sided', 'less', 'greater'}, optional.\n",
    "    Which alternative hypothesis to the null hypothesis the test uses.\n",
    "    Default is 'two-sided'.\n",
    "\n",
    "Returns\n",
    "* oddsratio: float.\n",
    "    This is prior odds ratio and not a posterior estimate.\n",
    "* p_value: float.\n",
    "    P-value, the probability of obtaining a distribution at least as\n",
    "    extreme as the one that was actually observed, assuming that the\n",
    "    null hypothesis is true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we spend a few days counting whales and sharks in the Atlantic and\n",
    "Indian oceans. In the Atlantic ocean we find 8 whales and 1 shark, in the\n",
    "Indian ocean 2 whales and 5 sharks. Then our contingency table is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.DataFrame([[8, 2],[1, 5]], \n",
    "                 index=['Atlantic', 'Indian'], \n",
    "                 columns=['whales', 'sharks'])\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use this table to find the p-value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "odds_ratio, p_value = scipy.stats.fisher_exact(f, alternative='two-sided')\n",
    "odds_ratio, p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probability that we would observe this or an even more imbalanced ratio\n",
    "by chance is about 3.5%.  A commonly used significance level is 5%--if we\n",
    "adopt that, we can therefore conclude that our observed imbalance is\n",
    "statistically significant; whales prefer the Atlantic while sharks prefer\n",
    "the Indian Ocean.\n",
    "\n",
    "For tables with large numbers, the (inexact) chi-square test implemented\n",
    "in the function `chi2_contingency` can also be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chi-square test of independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the chi-square test of independence when you have two nominal variables and you want to see whether the proportions of one variable are different for different values of the other variable. Use it when the sample size is large."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.chi2_contingency?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* observed: array_like.\n",
    "    The contingency table. The table contains the observed frequencies\n",
    "    (i.e. number of occurrences) in each category.  In the two-dimensional\n",
    "    case, the table is often described as an \"R x C table\".\n",
    "\n",
    "Returns:\n",
    "\n",
    "* chi2: float.\n",
    "    The test statistic.\n",
    "* p: float.\n",
    "    The p-value of the test\n",
    "* dof: int.\n",
    "    Degrees of freedom\n",
    "* expected: ndarray, same shape as `observed`.\n",
    "    The expected frequencies, based on the marginal sums of the table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2, p, dof, expected = scipy.stats.chi2_contingency(f)\n",
    "chi2, p, dof, expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-sample *t*-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-sample *t*-test is a two-sided test for the null hypothesis that the expected value\n",
    "(mean) of a sample of independent observations `a` is equal to the given\n",
    "population mean, `popmean`."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.ttest_1samp?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters:\n",
    "* a: array_like.\n",
    "    Sample observation\n",
    "* popmean: float or array_like.\n",
    "    Expected value in null hypothesis, if array_like than it must have the\n",
    "    same shape as `a` excluding the axis dimension\n",
    "    \n",
    "Returns:\n",
    "* statistic: float or array.\n",
    "    t-statistic\n",
    "* pvalue: float or array.\n",
    "    Two-tailed p-value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The population mean of moon-to-planet diameters (excluding our Moon) is approximately 0.018. Our moon is excluded from this population because its relative diameter of 0.273 is an outlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moon_planet_rel_diam_no_outliers.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **cannot** reject the null hypothesis that an expected mean of 0.02 is equal to population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_1samp(moon_planet_rel_diam_no_outliers, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **can** reject the null hypothesis that an expected mean equal to the Moon-Earth diamter ratio (0.273) is equal to population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.ttest_1samp(moon_planet_rel_diam_no_outliers, 0.273)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two-sample *t*-test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two-sample *t* test is a two-sided test for the null hypothesis that two independent samples\n",
    "have identical average (expected) values. This test assumes that the populations have identical variances by default."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.ttest_ind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "* a, b: array_like.\n",
    "    The arrays must have the same shape, except in the dimension\n",
    "    corresponding to `axis` (the first, by default).\n",
    "* axis : int or None, optional\n",
    "    Axis along which to compute test. If None, compute over the whole\n",
    "    arrays, `a`, and `b`.\n",
    "    \n",
    "Returns\n",
    "* statistic: float or array.\n",
    "    The calculated t-statistic.\n",
    "* pvalue: float or array.\n",
    "    The two-tailed p-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample data\n",
    "np.random.seed(1)\n",
    "a = np.random.randn(40)\n",
    "b = 1.5*np.random.randn(50)+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.stats.ttest_ind (t-test for the means of two independent samples)\n",
    "t, p = scipy.stats.ttest_ind(a, b, equal_var=False)\n",
    "print(\"ttest_ind: t = %g, p = %g\" % (t, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the descriptive statistics of a and b\n",
    "abar = a.mean()\n",
    "avar = a.var(ddof=1)\n",
    "na = a.size\n",
    "adof = na - 1\n",
    "\n",
    "bbar = b.mean()\n",
    "bvar = b.var(ddof=1)\n",
    "nb = b.size\n",
    "bdof = nb - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scipy.stats.ttest_ind_from_stats (t-test for the means of two independent samples from descriptive stats)\n",
    "t2, p2 = scipy.stats.ttest_ind_from_stats(abar, np.sqrt(avar), na,\n",
    "                              bbar, np.sqrt(bvar), nb,\n",
    "                              equal_var=False)\n",
    "print(\"ttest_ind_from_stats: t = %g, p = %g\" % (t2, p2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the formulas directly\n",
    "tf = (abar - bbar) / np.sqrt(avar/na + bvar/nb)\n",
    "dof = (avar/na + bvar/nb)**2 / (avar**2/(na**2*adof) + bvar**2/(nb**2*bdof))\n",
    "pf = 2*scipy.special.stdtr(dof, -np.abs(tf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the distributions\n",
    "sns.distplot(a, bins=10, label='a')\n",
    "sns.distplot(b, bins=10, label='b')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One-way anova"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The one-way ANOVA tests the null hypothesis that two or more groups have\n",
    "the same population mean.  The test is applied to samples from two or\n",
    "more groups, possibly with differing sizes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scipy.stats.f_oneway?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameters\n",
    "* sample1, sample2, ... : array_like.\n",
    "    The sample measurements for each group.\n",
    "\n",
    "Returns\n",
    "* statistic: float.\n",
    "    The computed F-value of the test.\n",
    "* pvalue: float.\n",
    "    The associated p-value from the F-distribution.\n",
    "\n",
    "Note: The ANOVA test has important assumptions that must be satisfied in order\n",
    "for the associated p-value to be valid.\n",
    "\n",
    "1. The samples are independent.\n",
    "2. Each sample is from a normally distributed population.\n",
    "3. The population standard deviations of the groups are all equal.  This\n",
    "   property is known as homoscedasticity.\n",
    "\n",
    "If these assumptions are not true for a given set of data, it may still be\n",
    "possible to use the Kruskal-Wallis H-test (`scipy.stats.kruskal`) although\n",
    "with some loss of power."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some data on a shell measurement (the length of the anterior\n",
    "adductor muscle scar, standardized by dividing by length) in the mussel\n",
    "*Mytilus trossulus* from five locations: Tillamook, Oregon; Newport, Oregon;\n",
    "Petersburg, Alaska; Magadan, Russia; and Tvarminne, Finland, taken from a\n",
    "much larger data set used in McDonald et al. (1991)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tillamook = [0.0571, 0.0813, 0.0831, 0.0976, 0.0817, 0.0859, 0.0735, 0.0659, 0.0923, 0.0836]\n",
    "newport = [0.0873, 0.0662, 0.0672, 0.0819, 0.0749, 0.0649, 0.0835, 0.0725]\n",
    "petersburg = [0.0974, 0.1352, 0.0817, 0.1016, 0.0968, 0.1064, 0.105]\n",
    "magadan = [0.1033, 0.0915, 0.0781, 0.0685, 0.0677, 0.0697, 0.0764, 0.0689]\n",
    "tvarminne = [0.0703, 0.1026, 0.0956, 0.0973, 0.1039, 0.1045]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(tillamook, label='Tillamook')\n",
    "sns.distplot(newport, label='Newport')\n",
    "sns.distplot(petersburg, label='Petersburg')\n",
    "sns.distplot(magadan, label='Magadan')\n",
    "sns.distplot(tvarminne, label='Tvarminne')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.stats.f_oneway(tillamook, newport, petersburg, magadan, tvarminne)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The means **were** significantly heterogeneous (one-way anova, $F_{4,34}=7.12, P=2.8\\times10^{-4}$). \n",
    "\n",
    "Complete ANOVA results (see http://www.biostathandbook.com/onewayanova.html):\n",
    "\n",
    ". | sum of squares | d.f. | mean square | Fs | P\n",
    "-----|-----|-----|-----|-----|-----\n",
    "among groups | 0.00452| 4 | 0.001113 | 7.12 | 2.8e-4\n",
    "within groups | 0.00539 | 34 | 0.000159 | | \n",
    "total | 0.00991 | 38 |  |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Microbiome analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This last lesson is taken from the documentation for [Scikit-bio](http://scikit-bio.org/docs/latest/). The package `skbio.diversity` provides diversity measures for OTU tables. OTUs are \"operational taxonomic units\"; you can think of them as taxa or species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio.diversity import alpha_diversity\n",
    "from skbio import TreeNode\n",
    "from io import StringIO\n",
    "from skbio.diversity import beta_diversity\n",
    "from skbio.stats.distance import mantel\n",
    "from skbio.stats.ordination import pcoa\n",
    "from skbio.stats.distance import anosim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a matrix containing 6 samples (rows) and 7 OTUs (columns):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [[23, 64, 14, 0, 0, 3, 1],\n",
    "        [0, 3, 35, 42, 0, 12, 1],\n",
    "        [0, 5, 5, 0, 40, 40, 0],\n",
    "        [44, 35, 9, 0, 1, 0, 0],\n",
    "        [0, 2, 8, 0, 35, 45, 1],\n",
    "        [0, 0, 25, 35, 0, 19, 0]]\n",
    "ids = list('ABCDEF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Alpha-diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we’ll compute observed OTUs, an alpha diversity metric, for each sample using the alpha_diversity driver function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "adiv_obs_otus = alpha_diversity('observed_otus', data, ids)\n",
    "adiv_obs_otus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll compute Faith’s PD on the same samples. Since this is a phylogenetic diversity metric, we’ll first create a tree and an ordered list of OTU identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = TreeNode.read(StringIO(\n",
    "               '(((((OTU1:0.5,OTU2:0.5):0.5,OTU3:1.0):1.0):0.0,'\n",
    "               '(OTU4:0.75,(OTU5:0.5,(OTU6:0.5,OTU7:0.5):0.5):'\n",
    "               '0.5):1.25):0.0)root;'))\n",
    "otu_ids = ['OTU1', 'OTU2', 'OTU3', 'OTU4', 'OTU5', 'OTU6', 'OTU7']\n",
    "adiv_faith_pd = alpha_diversity('faith_pd', data, ids=ids, otu_ids=otu_ids, tree=tree)\n",
    "adiv_faith_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beta-diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we’ll compute Bray-Curtis distances, a beta diversity metric, between all pairs of samples. Notice that the data and ids parameters provided to beta_diversity are the same as those provided to alpha_diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_dm = beta_diversity(\"braycurtis\", data, ids)\n",
    "print(bc_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll compute weighted UniFrac distances between all pairs of samples. Because weighted UniFrac is a phylogenetic beta diversity metric, we’ll need to pass the skbio.TreeNode and list of OTU ids that we created above. Again, these are the same values that were provided to alpha_diversity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wu_dm = beta_diversity(\"weighted_unifrac\", data, ids, tree=tree, otu_ids=otu_ids)\n",
    "print(wu_dm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we’ll do some work with these beta diversity distance matrices. First, we’ll determine if the UniFrac and Bray-Curtis distance matrices are significantly correlated by computing the Mantel correlation between them. Then we’ll determine if the p-value is significant based on an alpha of 0.05."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r, p_value, n = mantel(wu_dm, bc_dm)\n",
    "print(r)\n",
    "alpha = 0.05\n",
    "print(p_value)\n",
    "print(p_value < alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we’ll perform principal coordinates analysis (PCoA) on our weighted UniFrac distance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wu_pc = pcoa(wu_dm)\n",
    "print(wu_pc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCoA plots are only really interesting in the context of sample metadata, so let’s define some before we visualize these results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_md = [\n",
    "    ('A', ['gut', 's1']),\n",
    "    ('B', ['skin', 's1']),\n",
    "    ('C', ['tongue', 's1']),\n",
    "    ('D', ['gut', 's2']),\n",
    "    ('E', ['tongue', 's2']),\n",
    "    ('F', ['skin', 's2'])]\n",
    "sample_md = pd.DataFrame.from_items(sample_md, columns=['body_site', 'subject'], orient='index')\n",
    "sample_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s plot our PCoA results, coloring each sample by the subject it was taken from:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style('white')\n",
    "fig = wu_pc.plot(sample_md, 'subject',\n",
    "    axis_labels=('PC 1', 'PC 2', 'PC 3'),\n",
    "    title='Samples colored by subject', cmap='Set1', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don’t see any clustering/grouping of samples. If we were to instead color the samples by the body site they were taken from, we see that the samples from the same body site (those that are colored the same) appear to be closer to one another in the 3-D space then they are to samples from other body sites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = wu_pc.plot(sample_md, 'body_site',\n",
    "    axis_labels=('PC 1', 'PC 2', 'PC 3'),\n",
    "    title='Samples colored by body site', cmap='Set1', s=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordination techniques, such as PCoA, are useful for exploratory analysis. The next step is to quantify the strength of the grouping/clustering that we see in ordination plots. There are many statistical methods available to accomplish this; many operate on distance matrices. Let’s use ANOSIM to quantify the strength of the clustering we see in the ordination plots above, using our weighted UniFrac distance matrix and sample metadata.\n",
    "\n",
    "First test the grouping of samples by **subject**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = anosim(wu_dm, sample_md, column='subject', permutations=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['test statistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['p-value'] < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The negative value of ANOSIM’s R statistic indicates anti-clustering, but the p-value is insignificant at an alpha of 0.1.\n",
    "\n",
    "Now let’s test the grouping of samples by **body site**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = anosim(wu_dm, sample_md, column='body_site', permutations=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['test statistic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results['p-value'] < 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The R statistic indicates strong separation of samples based on body site. The p-value is significant at an alpha of 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlations with metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also explore the alpha diversity in the context of sample metadata. To do this, let’s add the Observed OTU and Faith PD data to our sample metadata. This is straight-forward beause alpha_diversity returns a Pandas Series object, and we’re representing our sample metadata in a Pandas DataFrame object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_md['Observed OTUs'] = adiv_obs_otus\n",
    "sample_md['Faith PD'] = adiv_faith_pd\n",
    "sample_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate these alpha diversity data in the context of our metadata categories. For example, we can generate boxplots showing Faith PD by body site."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = sample_md.boxplot(column='Faith PD', by='body_site')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute Spearman correlations between all pairs of columns in this DataFrame. Since our alpha diversity metrics are the only two numeric columns (and thus the only columns for which Spearman correlation is relevant), this will give us a symmetric 2x2 correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_md.corr(method=\"spearman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### R kernel in Jupyter notebooks\n",
    "\n",
    "Finally, if you want to use R in Jupyter notebooks, installation and instructions are here: <https://irkernel.github.io>."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
